{"cells":[{"cell_type":"markdown","metadata":{},"source":["# This notebook creates a very simple U-Net to segment clouds\n","Important remarks:\n","- The test-set is not being used\n","- The train set will be splitted into train and validation sets.\n","- The dataset creation is discussed in: https://medium.com/@cordmaur/how-to-create-a-custom-dataset-loader-in-pytorch-from-scratch-for-multi-band-satellite-images-c5924e908edf\n","- More information full explanation can can be found on the medium article: https://medium.com/@cordmaur/creating-a-very-simple-u-net-model-with-pytorch-for-semantic-segmentation-of-satellite-images-223aa216e705\n","- The training phase with 50 epochs takes around 3:30hs. "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from pathlib import Path\n","from torch.utils.data import Dataset, DataLoader, sampler\n","from PIL import Image\n","import torch\n","import matplotlib.pyplot as plt\n","import time"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from load_data import create_data_loaders\n","import time as time \n","import torchvision\n","import torch.nn.functional as F\n","import numpy as np \n","from collections import OrderedDict\n","\n","#https://amaarora.github.io/2020/09/13/unet.html\n","#https://www.pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/\n","\n","train_path = \"datasets-oxpet/train\"\n","validation_path = \"datasets-oxpet/val\"\n","test_path = \"datasets-oxpet/test\"\n","\n","\n","train_loader, validation_loader, test_loader = create_data_loaders(train_path, validation_path, test_path, batch_size=4)\n","\n","\n","class UNET(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","\n","        self.conv1 = self.contract_block(in_channels, 32, 7, 3)\n","        self.conv2 = self.contract_block(32, 64, 3, 1)\n","        self.conv3 = self.contract_block(64, 128, 3, 1)\n","\n","        self.upconv3 = self.expand_block(128, 64, 3, 1)\n","        self.upconv2 = self.expand_block(64*2, 32, 3, 1)\n","        self.upconv1 = self.expand_block(32*2, out_channels, 3, 1)\n","\n","    def __call__(self, x):\n","\n","        # downsampling part\n","        conv1 = self.conv1(x)\n","        conv2 = self.conv2(conv1)\n","        conv3 = self.conv3(conv2)\n","\n","        upconv3 = self.upconv3(conv3)\n","\n","        upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n","        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n","\n","        return upconv1\n","\n","    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n","\n","        contract = nn.Sequential(\n","            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n","            torch.nn.BatchNorm2d(out_channels),\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n","            torch.nn.BatchNorm2d(out_channels),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","                                 )\n","\n","        return contract\n","\n","    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n","\n","        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n","                            torch.nn.BatchNorm2d(out_channels),\n","                            torch.nn.ReLU(),\n","                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n","                            torch.nn.BatchNorm2d(out_channels),\n","                            torch.nn.ReLU(),\n","                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n","                            )\n","        return expand\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","opt = torch.optim.Adam(UNET.parameters(), lr=0.01)"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"ename":"TypeError","evalue":"__init__() missing 2 required positional arguments: 'in_channels' and 'out_channels'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/1366158838.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpredb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUNET\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'in_channels' and 'out_channels'"]}],"source":["import time\n","from IPython.display import clear_output\n","\n","def acc_metric(predb, yb):\n","    return (predb.argmax(dim=1) == yb.cuda()).float().mean()\n","\n","model = UNET()\n","\n","def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=1):\n","    start = time.time()\n","    model.cuda()\n","\n","    train_loss, valid_loss = [], []\n","\n","    best_acc = 0.0\n","\n","    for epoch in range(epochs):\n","        print('Epoch {}/{}'.format(epoch, epochs - 1))\n","        print('-' * 10)\n","\n","        for phase in ['train', 'valid']:\n","            if phase == 'train':\n","                model.train(True)  # Set trainind mode = true\n","                dataloader = train_dl\n","            else:\n","                model.train(False)  # Set model to evaluate mode\n","                dataloader = valid_dl\n","\n","            running_loss = 0.0\n","            running_acc = 0.0\n","\n","            step = 0\n","\n","            # iterate over data\n","            for x, y in dataloader:\n","                x = x.cuda()\n","                y = y.cuda()\n","                step += 1\n","\n","                # forward pass\n","                if phase == 'train':\n","                    # zero the gradients\n","                    optimizer.zero_grad()\n","                    outputs = model(x)\n","                    loss = loss_fn(outputs, y)\n","\n","                    # the backward pass frees the graph memory, so there is no \n","                    # need for torch.no_grad in this training pass\n","                    loss.backward()\n","                    optimizer.step()\n","                    # scheduler.step()\n","\n","                else:\n","                    with torch.no_grad():\n","                        outputs = model(x)\n","                        loss = loss_fn(outputs, y.long())\n","\n","                # stats - whatever is the phase\n","                acc = acc_fn(outputs, y)\n","\n","                running_acc  += acc*dataloader.batch_size\n","                running_loss += loss*dataloader.batch_size \n","\n","                if step % 100 == 0:\n","                    # clear_output(wait=True)\n","                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))\n","                    # print(torch.cuda.memory_summary())\n","\n","            epoch_loss = running_loss / len(dataloader.dataset)\n","            epoch_acc = running_acc / len(dataloader.dataset)\n","\n","            clear_output(wait=True)\n","            print('Epoch {}/{}'.format(epoch, epochs - 1))\n","            print('-' * 10)\n","            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n","            print('-' * 10)\n","\n","            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n","\n","    time_elapsed = time.time() - start\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n","    \n","    return train_loss, valid_loss    \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loss, valid_loss = train(unet, train_dl, valid_dl, loss_fn, opt, acc_metric, epochs=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,8))\n","plt.plot(train_loss, label='Train loss')\n","plt.plot(valid_loss, label='Valid loss')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def batch_to_img(xb, idx):\n","    img = np.array(xb[idx,0:3])\n","    return img.transpose((1,2,0))\n","\n","def predb_to_mask(predb, idx):\n","    p = torch.functional.F.softmax(predb[idx], 0)\n","    return p.argmax(0).cpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["xb, yb = next(iter(train_dl))\n","\n","with torch.no_grad():\n","    predb = unet(xb.cuda())\n","\n","predb.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bs = 12\n","fig, ax = plt.subplots(bs,3, figsize=(15,bs*5))\n","for i in range(bs):\n","    ax[i,0].imshow(batch_to_img(xb,i))\n","    ax[i,1].imshow(yb[i])\n","    ax[i,2].imshow(predb_to_mask(predb, i))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
